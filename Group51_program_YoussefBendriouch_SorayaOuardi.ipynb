{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e3a438d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkContext\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SQLContext\n\u001b[0;32m      3\u001b[0m sqlcontext \u001b[38;5;241m=\u001b[39m SQLContext(sc)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "sqlcontext = SQLContext(sc)\n",
    "\n",
    "# read data\n",
    "df = sqlContext.read.load(\"Chicago_Crimes_2012-2017.csv\", format='com.databricks.spark.csv', header = 'true', inferSchema='true')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c34121d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696b0f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17828cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bab9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data summary:\n",
    "df.describe().toPandas().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af59f8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unecessary columns:\n",
    "df = df.drop(\"Description\")\n",
    "df = df.drop(\"Unnamed: 0\")\n",
    "df = df.drop(\"ID\")\n",
    "df = df.drop(\"IUCR\")\n",
    "df = df.drop(\"Case Number\")\n",
    "df = df.drop(\"X Coordinate\")\n",
    "df = df.drop(\"Location\")\n",
    "df = df.drop(\"Y Coordinate\")\n",
    "df = df.drop(\"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60a2474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove missing values:\n",
    "df = df.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2634c93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates:\n",
    "df = df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558ac071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of rows\n",
    "df.count(),len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4787a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6de4eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with Primary type = 'NON-CRIMINAL (SUBJECT SPECIFIED)','NON-CRIMINAL' and 'NON - CRIMINAL'\n",
    "df = df.filter(df['Primary Type']!='NON-CRIMINAL (SUBJECT SPECIFIED)')\n",
    "df = df.filter(df['Primary Type']!='NON-CRIMINAL')\n",
    "df = df.filter(df['Primary Type']!='NON - CRIMINAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f287549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check:\n",
    "df.select(['Primary Type']).distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dfba51",
   "metadata": {},
   "source": [
    "# NB Model #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50aa6314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(df) for column in list(set(df.columns)-set(['Beat', 'District', 'Ward', 'Community Area', 'Year', 'Latitude', 'Longitude', 'Month', 'Day', 'Hour', 'Minute'])) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2e7331",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=indexers)\n",
    "df_r = pipeline.fit(df).transform(df)\n",
    "\n",
    "df_r.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d10d85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking:\n",
    "df_r.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78b67fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the Encoded columns\n",
    "df_r = df_r.drop(\"Primary Type\")\n",
    "df_r = df_r.drop(\"FBI Code\")\n",
    "df_r = df_r.drop(\"Arrest\")\n",
    "df_r = df_r.drop(\"Block\")\n",
    "df_r = df_r.drop(\"Location Description\")\n",
    "df_r = df_r.drop(\"Domestic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ef77de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_r.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1290d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert negative values from 'Longitude':\n",
    "from pyspark.sql.functions import abs\n",
    "\n",
    "df_r = df_r.withColumn('Longitude',abs(df_r.Longitude))\n",
    "df_r.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ad3ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column with all the features:\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "vectorAssembler = VectorAssembler(inputCols = [\n",
    " 'FBI Code_index',\n",
    " 'Primary Type_index',\n",
    " 'Block_index',\n",
    " 'Location Description_index',\n",
    " 'Domestic_index',\n",
    " 'Beat',\n",
    " 'District',\n",
    " 'Ward',\n",
    " 'Community Area',\n",
    " 'Year',\n",
    " 'Latitude',\n",
    " 'Longitude',\n",
    " 'Month',\n",
    " 'Day',\n",
    " 'Hour',\n",
    " 'Minute',],outputCol = \"features\") \n",
    "df_r_features = vectorAssembler.transform(df_r) \n",
    "df_r_features.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d8d6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check:\n",
    "df_r_features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e943382d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename \"Arrested_index\" to \"label\"\n",
    "df_r_features = df_r_features.withColumnRenamed(\"Arrest_index\",\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9154783e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check\n",
    "df_r_features.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ac087e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check\n",
    "df_r_features.select(['label']).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f5a7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the Data to train and test data:\n",
    "splits = df_r_features.randomSplit([0.7,0.3], 123) \n",
    "# optional value 42 is seed for sampling \n",
    "Training_df = splits[0] \n",
    "Testing_df = splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3dc35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive bayes:\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "NB = NaiveBayes(modelType=\"multinomial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f785f33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "nbmodel = NB.fit(Training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f6f90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict for df_r_features:\n",
    "NB_predictions_df = nbmodel.transform(Testing_df)\n",
    "#predictions_df.show(5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05e28b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_predictions_df.select('prediction','label').show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742c8d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of the NB model:\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"precision\") \n",
    "nbaccuracy = evaluator.evaluate(NB_predictions_df) \n",
    "print(\"The Naive Bayes model accuracy = \" + str(nbaccuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26358bcf",
   "metadata": {},
   "source": [
    "# Logistic Regression #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c884a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(df) for column in list(set(df.columns)-set(['Arrest','Beat', 'District', 'Ward', 'Community Area', 'Year', 'Latitude', 'Longitude', 'Month', 'Day', 'Hour', 'Minute'])) ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da3b7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=indexers)\n",
    "df_r_DT = pipeline.fit(df).transform(df)\n",
    "\n",
    "df_r_DT.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3695b45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check:\n",
    "df_r_DT.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248e45ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change arrest from boolean into string:\n",
    "from pyspark.sql.types import StringType,BooleanType,DateType\n",
    "from pyspark.sql.functions import col\n",
    "df_r_DT = df_r_DT.withColumn(\"Arrest\",col(\"Arrest\").cast(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd88962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# String indexing Arrest:\n",
    "indexer = StringIndexer(inputCol = \"Arrest\", outputCol = \"Arrest_index\")\n",
    "indexed = indexer.fit(df_r_DT).transform(df_r_DT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f258364f",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02e0909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Could be uncessary:\n",
    "# remove the Encoded columns\n",
    "indexed = indexed.drop(\"Primary Type\")\n",
    "df_r = df_r.drop(\"FBI Code\")\n",
    "df_r = df_r.drop(\"Arrest\")\n",
    "df_r = df_r.drop(\"Block\")\n",
    "df_r = df_r.drop(\"Location Description\")\n",
    "df_r = df_r.drop(\"Domestic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf72a5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert negative values from 'Longitude':\n",
    "from pyspark.sql.functions import abs\n",
    "\n",
    "indexed = indexed.withColumn('Longitude',abs(indexed.Longitude))\n",
    "indexed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3f6c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "# Create a column with all the features:\n",
    "vectorAssembler = VectorAssembler(inputCols = [\n",
    " 'FBI Code_index',\n",
    " 'Primary Type_index',\n",
    " 'Block_index',\n",
    " 'Location Description_index',\n",
    " 'Domestic_index',\n",
    " 'Beat',\n",
    " 'District',\n",
    " 'Ward',\n",
    " 'Community Area',\n",
    " 'Year',\n",
    " 'Latitude',\n",
    " 'Longitude',\n",
    " 'Month',\n",
    " 'Day',\n",
    " 'Hour',\n",
    " 'Minute',],outputCol = \"features\") \n",
    "df_r_features = vectorAssembler.transform(indexed) \n",
    "df_r_features.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05549d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename \"Arrested_index\" to \"label\"\n",
    "df_r_features = df_r_features.withColumnRenamed(\"Arrest_index\",\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303aa004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take only 'features' and 'labls'\n",
    "df_r2 = df_r_features.select('features','label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2264c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check:\n",
    "df_r2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1ba54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the Data to train and test data:\n",
    "splits = df_r2.randomSplit([0.6,0.4], 123) \n",
    "# optional value 42 is seed for sampling \n",
    "Training_df1 = splits[0] \n",
    "Testing_df1 = splits[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80620489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression:\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter=10)\n",
    "lrModel = lr.fit(Training_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2fe088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prdict:\n",
    "predictions = lrModel.transform(Testing_df1)\n",
    "predictions.select( 'label', 'rawPrediction', 'prediction', 'probability').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf02181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of logistic regression:\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "print('Test Area Under ROC', evaluator.evaluate(predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
